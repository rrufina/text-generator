{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "GPT3_model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0LgK5_3HWT",
        "outputId": "50f0c2a4-1580-44b8-b1f3-7eb2b50830cc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ZtCTtrdZ21mK"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskEgU_dDPev"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "jokes = pd.read_csv('shortjokes.csv')\n",
        "data_list = jokes['Joke'][:1000]\n",
        "data_list = data_list.tolist()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaDlYh5pEB92",
        "outputId": "ae2bae1c-5994-4ce2-f6f7-3fb8bdd79e78"
      },
      "source": [
        "print(len(data_list))\n",
        "count = 0.0\n",
        "max_len = 0\n",
        "for s in data_list:\n",
        "  lena = len(s.split())\n",
        "  count += lena\n",
        "  if lena > max_len:\n",
        "    max_len = lena\n",
        "print(count / len(data_list))\n",
        "print(max_len)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "217\n",
            "7.9953917050691246\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "phopRWoF21mM"
      },
      "source": [
        "data_list = set()\n",
        "with open('compl.txt','r') as f:\n",
        "    text = f.read()\n",
        "    text  =text.split('\\n')\n",
        "    data_list.update(text)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyntBPle21mM",
        "outputId": "da8e2239-895a-4cfd-923a-1ef5f3aeae5b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2gWED_78PH_"
      },
      "source": [
        "max_length = 10\n",
        "\n",
        "class ComplDataset(Dataset):\n",
        "    def __init__(self, compl, tokenizer, length):\n",
        "        self.compliments = []\n",
        "\n",
        "        for sent in data_list:\n",
        "            encod_dic = tokenizer('<SOS> ' + sent + ' <EOS>', truncation=True, max_length=length,\n",
        "                                  padding='max_length')\n",
        "            self.compliments.append(torch.tensor(encod_dic['input_ids']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.compliments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.compliments[idx]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpI5GUgc8ulk",
        "outputId": "fdfc3631-490c-4d41-d491-6b041cd45546"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<SOS>', eos_token='<EOS>', pad_token = '<EOS>')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrxfH3Pf9VXa"
      },
      "source": [
        "dataset = ComplDataset(data_list, tokenizer, max_length)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdcVN4qhDRjz"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_ids, val_ids = train_test_split(\n",
        "    np.arange(len(dataset)),\n",
        "    test_size=0.1,\n",
        "    shuffle=True)\n",
        "\n",
        "train = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "val = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=train)\n",
        "val_dataloader =  torch.utils.data.DataLoader(dataset, batch_size=1, sampler=val)\n",
        "\n",
        "train_size = len(train_ids)\n",
        "val_size = len(val_ids)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HevHNPhX21mO"
      },
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Used documentation: https://huggingface.co/transformers/model_doc/gpt2.html\n",
        "\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=5000, num_training_steps = -1)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEXlCtay21mP",
        "outputId": "37320036-0c33-4e10-e839-50b5b58a9c91"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(10):\n",
        "    epoch_loss_train = 0\n",
        "    model.train()\n",
        "    for i, entity in enumerate(tqdm(train_dataloader)):\n",
        "        input_ids = entity.to(device)\n",
        "\n",
        "        outputs = model(input_ids, labels = input_ids)\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        epoch_loss_train += batch_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    epoch_loss_val = 0\n",
        "\n",
        "    for entity in tqdm(val_dataloader):\n",
        "        with torch.no_grad():\n",
        "            input_ids = entity.to(device)\n",
        "\n",
        "\n",
        "            outputs = model(input_ids, labels = input_ids)\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        epoch_loss_val += batch_loss\n",
        "\n",
        "    print('Average train loss: {}'.format(epoch_loss_train / train_size))\n",
        "    print('Average val loss: {}'.format(epoch_loss_val / val_size))\n",
        "    torch.save(model.state_dict(), 'models/GPT2.h5')"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.16it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 46.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 57.7973605180398\n",
            "Average val loss: 11.142436894503506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.20it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 44.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 8.991546473136315\n",
            "Average val loss: 6.440600373528221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.22it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 45.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 6.169624839684902\n",
            "Average val loss: 5.465569452805952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.20it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 46.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 5.303812132126246\n",
            "Average val loss: 5.087062868204984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.22it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 42.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 4.6253371397654215\n",
            "Average val loss: 5.048749349334023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.20it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 44.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 4.344366181202424\n",
            "Average val loss: 5.038425028324127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.19it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 46.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 4.361417638032864\n",
            "Average val loss: 5.047712136398662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.19it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 43.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 3.911240833844894\n",
            "Average val loss: 5.009311107071963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.18it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 44.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 3.8499731589586306\n",
            "Average val loss: 5.043284947221929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [00:27<00:00,  7.18it/s]\n",
            "100%|██████████| 22/22 [00:00<00:00, 45.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 3.7998245055858906\n",
            "Average val loss: 5.0875240401788195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL5Bh2dX21mP",
        "outputId": "3b16c285-4373-4b53-fd66-61fb1ab3153d"
      },
      "source": [
        "state_dict = torch.load('models/GPT2.h5')\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9K1jhX621mQ",
        "outputId": "26d5ff59-94cd-4505-853c-607686ab4419"
      },
      "source": [
        "model.eval()\n",
        "generated = torch.tensor(tokenizer.encode('<SOS>')).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated, \n",
        "                                do_sample=True,   \n",
        "                                top_k=50, \n",
        "                                max_length = 20,\n",
        "                                top_p=0.95, \n",
        "                                bos_token = '<SOS>',\n",
        "                                eos_token = '<EOS>',\n",
        "                                num_return_sequences=5\n",
        "                                )\n",
        "\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True).replace('\\n',' ')))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50257]], device='cuda:0')\n",
            "0:  You are incredible to a great. \n",
            "\n",
            "\n",
            "1:  You're always irresistible to. \n",
            "\n",
            "\n",
            "2:  You be always is better than a beautiful. \n",
            "\n",
            "\n",
            "3:  You would special the awesome. \n",
            "\n",
            "\n",
            "4:  You are really awesome. \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
