{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "markov.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nV5Rc9sJV0XO",
        "Aw-HeqfoV0XS",
        "j3BTq1vyV0XU",
        "rb7uoPBjV0XV",
        "yLbe1YJDV0XW"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-AcH8AW9Ypp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634de355-64e5-4724-8786-b361aad96be1"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import os, shutil\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkAeVpN-8eqp"
      },
      "source": [
        "path = '/content/dataset.txt'\n",
        "with open(path, \"r\") as f:\n",
        "  text = f.read().lower()\n",
        "\n",
        "text = re.sub(\"\\n\", \" EOS SOS \", text)\n",
        "text = re.sub(\"\\.\", \"\", text)\n",
        "text = \"SOS \"+text+\" EOS\"\n",
        "dataset = word_tokenize(text)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adT-aof4Bm1O",
        "outputId": "720802b9-de54-47a2-c5a5-d142542c8413"
      },
      "source": [
        "#print('Number of samples:', total)\n",
        "print('Number of words:', len(dataset))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 10763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs7SApIeW4AU"
      },
      "source": [
        "def generateTable(data):\n",
        "    table = {}\n",
        "    for i in range(len(data)-1):\n",
        "        X = data[i]\n",
        "        Y = data[i+1]\n",
        "\n",
        "        if X not in table.keys():\n",
        "            table[X] = {}\n",
        "            table[X][Y] = 1\n",
        "        else:\n",
        "            if Y not in table[X].keys():\n",
        "                table[X][Y] = 1\n",
        "            else:\n",
        "                table[X][Y] += 1\n",
        "    \n",
        "    return table\n",
        "\n",
        "def convertFreqIntoProb(table):     \n",
        "    for word in table.keys():\n",
        "        s = float(sum(table[word].values()))\n",
        "        for prob in table[word].keys():\n",
        "            table[word][prob] = table[word][prob]/s\n",
        "    return table"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIw2L0kNXddt"
      },
      "source": [
        "table = generateTable(dataset)\n",
        "chain = convertFreqIntoProb(table)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCB-TotkXg6h"
      },
      "source": [
        "def sample_next(word,chain):\n",
        "    if word not in chain.keys():\n",
        "        return \"\\n\"\n",
        "    following_words = list(chain[word].keys())\n",
        "    probs = list(chain[word].values())\n",
        "\n",
        "    mean = np.mean(probs)\n",
        "    best_next = []\n",
        "    for var in following_words:\n",
        "        if chain[word][var] > mean:\n",
        "            best_next.append(var)\n",
        "\n",
        "    if len(best_next) == 0:\n",
        "        best_next = following_words\n",
        "    \n",
        "    return np.random.choice(best_next)\n",
        " "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWjVIj4iXkTC"
      },
      "source": [
        "def generateText(first_word, chain, maxLen=100):\n",
        "\n",
        "    sentence = [first_word]\n",
        "    target_word = first_word\n",
        "    \n",
        "    for i in range(maxLen):\n",
        "        next_prediction = sample_next(target_word,chain)\n",
        "        sentence.append(next_prediction)\n",
        "        target_word = sentence[-1]\n",
        "        if target_word == 'EOS':\n",
        "            break\n",
        "    \n",
        "    sentence = sentence[1:]\n",
        "    sentence[-1] = \".\"\n",
        "    text = re.sub(r'\\s+([?.,!’\"])', r'\\1', \" \".join(sentence))\n",
        "    return text.capitalize()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsMaq8ve9sK0",
        "outputId": "af4bdd54-1ae5-45b1-c916-ba8c48afc1ca"
      },
      "source": [
        "for i in range(20):\n",
        "  print(f'{i+1}. {generateText(\"SOS\",chain)}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Your voice is always be without you and always.\n",
            "2. It would have the only person i promise to the happiness in the rest of my life because you came into my life, but with your eyes, and all i was until i need to hold on the first day, i would be without you.\n",
            "3. The darkness and encouragement that we will always be so much i feel alive and yet here i bet you and keep going.\n",
            "4. And funny… this world.\n",
            "5. Your hair and stars may not have a happy and every time i need to keep your hair and you for a mother.\n",
            "6. My whole day, and love you are always learning new things and night.\n",
            "7. The first sight until i would have given me? even when you were possible for all i will be impossible for that we became the love, the only one of life, so much i say that is because you know you in life, i just wanted to let you know how i promise to make you and always look great deal to tell you so many years.\n",
            "8. You in life with your face when i would have been searching for all that is to hold on my whole day.\n",
            "9. But with your hair looks stunning.\n",
            "10. My mind, my mind and your perspective on me feel so special, and stars may not say it would have become.\n",
            "11. It is an ethereal beauty that you as my love you, your perspective on your life with all the first saw you with all my dream come true.\n",
            "12. Thank you know you is impressive.\n",
            "13. How to my side in your face when you and keep going.\n",
            "14. And encouragement that no choice but because you.\n",
            "15. There is everything i bet you in this world to me a mother like you.\n",
            "16. And your passion swell up my side.\n",
            "17. I feel so thankful for me a kind to let you.\n",
            "18. The one.\n",
            "19. How i found you will always have you do n't like a grain of my side.\n",
            "20. The brightest star that you forever.\n"
          ]
        }
      ]
    }
  ]
}